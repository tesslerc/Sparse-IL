{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import gensim\n",
    "import gensim.downloader as glove_api\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "\n",
    "from ZorkGym.text_utils.text_parser import BagOfWords, Word2Vec, TextParser, tokenizer, BasicParser\n",
    "from networks.cnn import TextCNN\n",
    "from agents.OMP_DDPG import OMPDDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sys\n",
    "from scipy.linalg import norm\n",
    "from math import sqrt\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hashlib import sha1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module implementing the FISTA algorithm\n",
    "\"\"\"\n",
    "__author__ = 'Jean KOSSAIFI'\n",
    "\n",
    "\n",
    "def mixed_norm(coefs, p, q=None, n_samples=None, n_kernels=None):\n",
    "    \"\"\" Computes the (p, q) mixed norm of the vector coefs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coefs : ndarray\n",
    "        a vector indexed by (l, m)\n",
    "        with l in range(0, n_kernels)\n",
    "            and m in range(0, n_samples)\n",
    "\n",
    "    p : int or np.inf\n",
    "\n",
    "    q : int or np.int\n",
    "\n",
    "    n_samples : int, optional\n",
    "        number of elements in each kernel\n",
    "        default is None\n",
    "\n",
    "    n_kernels : int, optional\n",
    "        number of kernels\n",
    "        default is None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    if q is None or p == q:\n",
    "        return norm(coefs, p)\n",
    "    else:\n",
    "        return norm([norm(i, p) for i in coefs.reshape(\n",
    "            n_kernels, n_samples)], q)\n",
    "\n",
    "\n",
    "def dual_mixed_norm(coefs, n_samples, n_kernels, norm_):\n",
    "    \"\"\" Returns a function corresponding to the dual mixt norm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coefs : ndarray\n",
    "        a vector indexed by (l, m)\n",
    "        with l in range(0, n_kernels)\n",
    "            and m in range(0, n_samples)\n",
    "\n",
    "    n_samples : int, optional\n",
    "        number of elements in each kernel\n",
    "        default is None\n",
    "\n",
    "    n_kernels : int, optional\n",
    "        number of kernels\n",
    "        default is None\n",
    "\n",
    "    norm_ : {'l11', 'l12', 'l21', 'l22'}\n",
    "        the dual mixed norm we want to compute\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    if norm_ == 'l11':\n",
    "        res = norm(coefs, np.inf)\n",
    "    elif norm_ == 'l12':\n",
    "        res = mixed_norm(coefs, np.inf, 2, n_samples, n_kernels)\n",
    "    elif norm_ == 'l21':\n",
    "        res = mixed_norm(coefs, 2, np.inf, n_samples, n_kernels)\n",
    "    else:\n",
    "        res = norm(coefs, 2)\n",
    "    return res\n",
    "\n",
    "\n",
    "def by_kernel_norm(coefs, p, q, n_samples, n_kernels):\n",
    "    \"\"\" Computes the (p, q) norm of coefs for each kernel\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coefs : ndarray\n",
    "        a vector indexed by (l, m)\n",
    "        with l in range(0, n_kernels)\n",
    "            and m in range(0, n_samples)\n",
    "\n",
    "    p : int or np.inf\n",
    "\n",
    "    q : int or np.inf\n",
    "\n",
    "    n_samples : int, optional\n",
    "        number of elements in each kernel\n",
    "        default is None\n",
    "\n",
    "    n_kernels : int, optional\n",
    "        number of kernels\n",
    "        default is None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list of the norms of the sub vectors associated to each kernel\n",
    "    \"\"\"\n",
    "    return [mixed_norm(i, p, q, n_samples, 1)\n",
    "            for i in coefs.reshape(n_kernels, n_samples)]\n",
    "\n",
    "\n",
    "def prox_l11(u, lambda_):\n",
    "    \"\"\" Proximity operator for l(1, 1, 2) norm\n",
    "\n",
    "    \n",
    "\n",
    "    :math:`\\\\hat{\\\\alpha}_{l,m} = sign(u_{l,m})\\\\left||u_{l,m}| - \\\\lambda \\\\right|_+`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    u : ndarray\n",
    "        The vector (of the n-dimensional space) on witch we want\n",
    "        to compute the proximal operator\n",
    "\n",
    "    lambda_ : float\n",
    "        regularisation parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray : the vector corresponding to the application of the\n",
    "             proximity operator to u\n",
    "\n",
    "    \"\"\"\n",
    "    return np.sign(u) * np.maximum(np.abs(u) - lambda_, 0.)\n",
    "\n",
    "def prox_l22(u, lambda_):\n",
    "    \"\"\" proximity operator l(2, 2, 2) norm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "     u : ndarray\n",
    "        The vector (of the n-dimensional space) on witch we want to compute the proximal operator\n",
    "\n",
    "    lambda_ : float\n",
    "        regularisation parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    ndarray : the vector corresponding to the application of the proximity operator to u\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    :math:`\\\\hat{\\\\alpha}_{l,m} = \\\\frac{1}{1 + \\\\lambda} \\\\, u_{l,m}`\n",
    "\n",
    "    \"\"\"\n",
    "    return 1./(1.+lambda_)*u\n",
    "\n",
    "def prox_l21_1(u, l, n_samples, n_kernels):\n",
    "    \"\"\" Proximity operator l(2, 1, 1) norm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    u : ndarray\n",
    "        The vector (of the n-dimensional space) on witch we want to compute the proximal operator\n",
    "\n",
    "    lambda_ : float\n",
    "        regularisation parameter\n",
    "    \n",
    "    n_samples : int, optional\n",
    "        number of elements in each kernel\n",
    "        default is None\n",
    "\n",
    "    n_kernels : int, optional\n",
    "        number of kernels\n",
    "        default is None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray : the vector corresponding to the application of the proximity operator to u\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "    .. math::\n",
    "\n",
    "       \\hat{\\alpha}_{l,m} = u_{l,m} \\left| 1 - \\frac{\\lambda}{\\|u_{l \\bullet}\\|_{2}} \\right|_+\\\n",
    "\n",
    "    where l is in range(0, n_samples) and m is in range(0, n_kernels)\n",
    "    so :math:`u_{l\\\\bullet}` = [u(l, m) for m in n_kernels]\n",
    "\n",
    "    \"\"\"\n",
    "    return (u.reshape(n_kernels, n_samples) *\\\n",
    "        [max(1. - l/norm(u[np.arange(n_kernels)*n_samples+i], 2), 0.)\n",
    "            for i in range(n_samples)]).reshape(-1)\n",
    "\n",
    "\n",
    "def prox_l21(u, l, n_samples, n_kernels):\n",
    "    \"\"\" proximity operator l(2, 1, 2) norm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    u : ndarray\n",
    "        The vector (of the n-dimensional space) on witch we want to compute the proximal operator\n",
    "\n",
    "    lambda_ : float\n",
    "        regularisation parameter\n",
    "\n",
    "    n_samples : int, optional\n",
    "        number of elements in each kernel\n",
    "        default is None\n",
    "\n",
    "    n_kernels : int, optional\n",
    "        number of kernels\n",
    "        default is None\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray : the vector corresponding to the application of the proximity operator to u\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    :math:`\\\\hat{\\\\alpha}_{l,m} = u_{l,m} \\\\left| 1 - \\\\frac{ \\\\lambda}{ \\\\|u_{l \\\\bullet }\\\\|_{2}} \\\\right|_+`\n",
    "\n",
    "    where l is in range(0, n_kernels) and m is in range(0, n_samples)\n",
    "    so :math:`u_{l \\\\bullet }` = [u(l, m) for l in n_samples]\n",
    "\n",
    "    \"\"\"\n",
    "    for i in u.reshape(n_kernels, n_samples):\n",
    "        n = norm(i, 2)\n",
    "        if n==0 or n==np.Inf:\n",
    "            i[:] = 0\n",
    "        else:\n",
    "            i[:] *=  max(1. - l/n, 0.)\n",
    "        # !! If you do just i *= , u isn't modified\n",
    "        # The slice is needed here so that the array can be modified\n",
    "    return u\n",
    "\n",
    "\n",
    "def prox_l12(u, l, n_samples, n_kernels):\n",
    "    \"\"\" proximity operator for l(1, 2, 2) norm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    u : ndarray\n",
    "        The vector (of the n-dimensional space) on witch we want to compute the proximal operator\n",
    "\n",
    "    lambda_ : float\n",
    "        regularisation parameter\n",
    "\n",
    "    n_samples : int, optional\n",
    "        number of elements in each kernel\n",
    "        default is None\n",
    "\n",
    "    n_kernels : int, optional\n",
    "        number of kernels\n",
    "        default is None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray : the vector corresponding to the application of the proximity operator to u\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    :math:`\\\\hat{\\\\alpha}_{l,m} = sign(u_{l,m})\\\\left||u_{l,m}| - \\\\frac{\\\\lambda \\\\sum\\\\limits_{m_l=1}^{M_l} u2_{l,m_l}}{(1+\\\\lambda M_l) \\\\|u_{l \\\\bullet }\\\\|_{2}} \\\\right|_+`\n",
    "\n",
    "    where  :math:`u2_{l,m_l}`  denotes the :math:`|u_{l,m_l}|`\n",
    "        ordered  by descending  order for fixed  :math:`l`,  and the\n",
    "            quantity :math:`M_l` is the number computed in compute_M\n",
    "\n",
    "    \"\"\"\n",
    "    for i in u.reshape(n_kernels, n_samples):\n",
    "        Ml, sum_Ml = compute_M(i, l, n_samples)\n",
    "        # i[:] so that u is really modified\n",
    "        n = norm(i, 2)\n",
    "        if n == 0 or n == np.Inf:\n",
    "            i[:] = 0\n",
    "        else:\n",
    "            i[:] = np.sign(i)*np.maximum(\n",
    "                np.abs(i)-(l*sum_Ml)/((1.+l*Ml)*n), 0.)\n",
    "    return u\n",
    "\n",
    "def compute_M(u, lambda_, n_samples):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    u : ndarray \n",
    "        ndarray of size (n_samples * n_samples) representing a subvector of K,\n",
    "        ie the samples for a single kernel\n",
    "\n",
    "    lambda_ : int\n",
    "\n",
    "    n_samples : int\n",
    "        number of elements in each kernel \n",
    "        ie number of elements of u\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "    :math:`M_l` is the number such that\n",
    "\n",
    "    :math:`u2_{l,M_l+1} \\\\leq  \\\\lambda \\\\sum_{m_l=1}^{M_l+1} \\\\left( u2_{l,m_l} - u2_{l,M_l+1}\\\\right)`\n",
    "\n",
    "    and\n",
    "\n",
    "\n",
    "    :math:`u2_{l,M_l} > \\\\lambda\\\\sum_{m_l=1}^{M_l} \\\\left( u2_{l,m_l} - u2_{k,M_l}\\\\right)`\n",
    "\n",
    "    Detailed explication\n",
    "    \n",
    "    let u denotes |u(l)|, the vector associated with the kernel l, ordered by descending order\n",
    "    Ml is the integer such that\n",
    "        u(Ml) <= l * sum(k=1..Ml + 1) (u(k) - u(Ml + 1))    (S1)\n",
    "        and\n",
    "        u(Ml) > l * sum(k=1..Ml) (u(k) - u(Ml)              (S2)\n",
    "    Note that in that definition, Ml is in [1..Ml]\n",
    "    In python, while Ml is in [1..(Ml-1)], indices will be in [0..(Ml-1)], so we must take care of indices.\n",
    "    That's why, we consider Ml is in [0..(Ml-1)] and, at the end, we add 1 to the result\n",
    "\n",
    "    Detailed example\n",
    "\n",
    "    if u(l) = [0 1 2 3] corrsponds to the vector associated with a kernel\n",
    "        then u = |u(l)| ordered by descending order ie u = [3 2 1 0]\n",
    "\n",
    "    Then u = [3 2 1 0]\n",
    "    let l = 1\n",
    "    Ml is in {0, 1, 2} (not 3 because we also consider Ml+1)\n",
    "    # Note : in fact Ml is in {1, 2, 3} but it is more convenient\n",
    "    # to consider it is in {0, 1, 2} as indexing in python starts at 0\n",
    "    # We juste have to add 1 to the final result\n",
    "\n",
    "    if Ml = 0 then S1 = 1 and S2 = 0\n",
    "    if Ml = 1 then S1 = 3 and S2 = 1\n",
    "    if Ml = 2 then S1 = 6 and S2 = 3\n",
    "\n",
    "    if Ml = 0 then u(Ml+1)=u(1)=2  > l*... =1  (S1 is not verified)\n",
    "              and  u(Ml)=u(0)=3    > l*... =0  (S2 is verified)\n",
    "\n",
    "    if Ml = 1 then u(Ml+1)=u(2)=1 <= l*... =3  (S1 is verified)\n",
    "              and  u(Ml)=u(1)=2    > l*... =1  (S2 is verified)\n",
    "\n",
    "    if Ml = 2 then u(Ml+1)=u(3)=0 <= l*... =6  (S1 is verified)\n",
    "              but  u(Ml)=u(2)=1   <= l*... =3  (S1 is not verified)\n",
    "\n",
    "    Conclusion : Ml = 1 + 1 !!\n",
    "    Ml = 2 because in python, indexing starts at 0, so Ml +1\n",
    "\n",
    "    \"\"\"\n",
    "    u = np.sort(np.abs(u))[::-1]\n",
    "    S1 = u[1:] - lambda_*(np.cumsum(u)[:-1] - (np.arange(n_samples-1)+1)*u[1:])\n",
    "    S2 = u[:-1] - lambda_*(np.cumsum(u)[:-1] - (np.arange(n_samples-1)+1)*u[:-1])\n",
    "    Ml = np.argmax((S1<=0.) & (S2>0.)) + 1\n",
    "\n",
    "    return Ml, np.sum(u[:Ml]) # u[:Ml] = u[0, 1, ..., Ml-1] !!\n",
    "\n",
    "\n",
    "def hinge_step(y, K, Z):\n",
    "    \"\"\"\n",
    "    Returns the point in witch we apply gradient descent\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    y : np-array\n",
    "        the labels vector\n",
    "\n",
    "    K : 2D np-array\n",
    "        the concatenation of all the kernels, of shape\n",
    "        n_samples, n_kernels*n_samples\n",
    "\n",
    "    Z : a linear combination of the last two coefficient vectors\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    res : np-array of shape n_samples*,_kernels\n",
    "          a point of the space where we will apply gradient descent\n",
    "    \"\"\"\n",
    "    return np.dot(K.transpose(), np.maximum(1 - np.dot(K, Z), 0))\n",
    "\n",
    "def least_square_step(y, K, Z):\n",
    "    \"\"\"\n",
    "    Returns the point in witch we apply gradient descent\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    y : np-array\n",
    "        the labels vector\n",
    "\n",
    "    K : 2D np-array\n",
    "        the concatenation of all the kernels, of shape\n",
    "        n_samples, n_kernels*n_samples\n",
    "\n",
    "    Z : a linear combination of the last two coefficient vectors\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    res : np-array of shape n_samples*,_kernels\n",
    "          a point of the space where we will apply gradient descent\n",
    "    \"\"\"\n",
    "    return np.dot(K.transpose(), y - np.dot(K,Z))\n",
    "\n",
    "\n",
    "def _load_Lipschitz_constant(K):\n",
    "    \"\"\" Loads the Lipschitz constant and computes it if not already saved\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    K : 2D-ndarray\n",
    "        The matrix of witch we want to compute the Lipschitz constant\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Lipshitz constant is just a number < 2/norm(np.dot(K, K.T), 2)\n",
    "\n",
    "    The constant is stored in a npy hidden file, in the current directory.\n",
    "    The filename is the sha1 hash of the ndarray\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mu = np.load('./.%s.npy' % sha1(K).hexdigest())\n",
    "    except:\n",
    "        mu = 1/norm(np.dot(K, K.transpose()), 2)\n",
    "        np.save('./.%s.npy' % sha1(K).hexdigest(), mu)\n",
    "    return mu\n",
    "    \n",
    "\n",
    "class Fista(BaseEstimator):\n",
    "    \"\"\"\n",
    "\n",
    "    Fast iterative shrinkage/thresholding Algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    lambda_ : int, optionnal\n",
    "        regularisation parameter\n",
    "        default is 0.5\n",
    "\n",
    "    loss : {'squared-hinge', 'least-square'}, optionnal\n",
    "        the loss function to use\n",
    "        defautl is 'squared-hinge'\n",
    "        \n",
    "    penalty : {'l11', 'l22', 'l12', 'l21'}, optionnal\n",
    "        norm to use as penalty\n",
    "        default is l11\n",
    "\n",
    "    n_iter : int, optionnal\n",
    "        number of iterations\n",
    "        default is 1000\n",
    "\n",
    "    recompute_Lipschitz_constant : bool, optionnal\n",
    "        if True, the Lipschitz constant is recomputed everytime\n",
    "        if False, it is stored based on it's sha1 hash\n",
    "        default is False\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda_=0.5, loss='squared-hinge', penalty='l11', n_iter=1000, recompute_Lipschitz_constant=False):\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_ = lambda_\n",
    "        self.loss = loss\n",
    "        self.penalty = penalty\n",
    "        self.p = int(penalty[1])\n",
    "        self.q = int(penalty[2])\n",
    "        self.recompute_Lipschitz_constant = recompute_Lipschitz_constant\n",
    "\n",
    "    def fit(self, K, y, Lipschitz_constant=None,  verbose=0, **params):\n",
    "        \"\"\" Fits the estimator\n",
    "\n",
    "        We want to solve a problem of the form y = KB + b\n",
    "            where K is a (n_samples, n_kernels*n_samples) matrix.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        K : ndarray\n",
    "            numpy array of shape (n, p)\n",
    "            K is the concatenation of the p/n kernels\n",
    "                where each kernel is of size (n, n)\n",
    "\n",
    "        y : ndarray\n",
    "            an array of the labels to predict for each kernel\n",
    "            y is of size p\n",
    "                where K.shape : (n, p)\n",
    "\n",
    "        Lipschitz_constant : float, optionnal\n",
    "             allow the user to pre-compute the Lipschitz constant\n",
    "             (its computation can be very slow, so that parameter is very\n",
    "             usefull if you were to use several times the algorithm on the same data)\n",
    "\n",
    "        verbose : {0, 1}, optionnal\n",
    "            verbosity of the method : 1 will display informations while 0 will display nothing\n",
    "            default = 0\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        next_step = hinge_step\n",
    "        if self.loss=='squared-hinge':\n",
    "            K = y[:, np.newaxis] * K\n",
    "            # Equivalent to K = np.dot(np.diag(y), X) but faster\n",
    "        elif self.loss=='least-square':\n",
    "            next_step = least_square_step\n",
    "\n",
    "        (n_samples, n_features) = K.shape\n",
    "        n_kernels = int(n_features/n_samples) # We assume each kernel is a square matrix\n",
    "        self.n_samples, self.n_kernels = n_samples, n_kernels\n",
    "\n",
    "        if Lipschitz_constant==None:\n",
    "            Lipschitz_constant = _load_Lipschitz_constant(K)\n",
    "\n",
    "        tol = 10**(-6)\n",
    "        coefs_current = np.zeros(n_features, dtype=np.float) # coefficients to compute\n",
    "        coefs_next = np.zeros(n_features, dtype=np.float)\n",
    "        Z = np.copy(coefs_next) # a linear combination of the coefficients of the 2 last iterations\n",
    "        tau_1 = 1\n",
    "\n",
    "        if self.penalty=='l11':\n",
    "            prox = lambda u:prox_l11(u, self.lambda_*Lipschitz_constant)\n",
    "        elif self.penalty=='l22':\n",
    "            prox = lambda u:prox_l22(u, self.lambda_*Lipschitz_constant)\n",
    "        elif self.penalty=='l21':\n",
    "            prox = lambda u:prox_l21(u, self.lambda_*Lipschitz_constant, n_samples, n_kernels)\n",
    "        elif self.penalty=='l12':\n",
    "            prox = lambda u:prox_l12(u, self.lambda_*Lipschitz_constant, n_samples, n_kernels)\n",
    "\n",
    "        if verbose==1:\n",
    "            self.iteration_dual_gap = list()\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            coefs_current = coefs_next # B_(k-1) = B_(k)\n",
    "            coefs_next = prox(Z + Lipschitz_constant*next_step(y, K, Z))\n",
    "            \n",
    "            tau_0 = tau_1 #tau_(k+1) = tau_k\n",
    "            tau_1 = (1 + sqrt(1 + 4*tau_0**2))/2\n",
    "\n",
    "            Z = coefs_next + (tau_0 - 1)/tau_1*(coefs_next - coefs_current)\n",
    "            \n",
    "            # Dual problem\n",
    "            objective_var = 1 - np.dot(K, coefs_next)\n",
    "            objective_var = np.maximum(objective_var, 0) # Shrink\n",
    "            # Primal objective function\n",
    "            penalisation = self.lambda_/self.q*(mixed_norm(coefs_next,\n",
    "                    self.p, self.q, n_samples, n_kernels)**self.q)\n",
    "            loss = 0.5*np.sum(objective_var**2)\n",
    "            objective_function = penalisation + loss\n",
    "\n",
    "            # Dual objective function\n",
    "            dual_var = objective_var\n",
    "            if self.lambda_ != 0:\n",
    "                dual_penalisation = dual_mixed_norm(np.dot(K.T,dual_var)/self.lambda_,\n",
    "                        n_samples, n_kernels, self.penalty)\n",
    "                if self.q==1:\n",
    "                    # Fenchel conjugate of a mixed norm\n",
    "                    if dual_penalisation > 1:\n",
    "                        dual_var = dual_var / dual_penalisation\n",
    "                        # If we did not normalise, dual_penalisation\n",
    "                        # would be +infinity ...\n",
    "                    dual_penalisation = 0\n",
    "                else:\n",
    "                    # Fenchel conjugate of a squared mixed norm\n",
    "                    dual_penalisation = self.lambda_/2*(dual_penalisation**2)\n",
    "            else:\n",
    "                dual_penalisation = 0\n",
    "            dual_loss = -0.5*np.sum(dual_var**2) + np.sum(dual_var)\n",
    "            # trace(np.dot(duat_var[:, np.newaxis], y)) au lieu du sum(dual_var) ?\n",
    "            dual_objective_function = dual_loss - self.lambda_/self.q*dual_penalisation\n",
    "            gap = abs(objective_function - dual_objective_function)\n",
    "\n",
    "            if verbose:\n",
    "                sys.stderr.write(\"Iteration : %d\\r\" % i )\n",
    "                # print \"iteration %d\" % i\n",
    "                self.iteration_dual_gap.append(gap)\n",
    "                if i%1000 == 0:\n",
    "                    print(\"primal objective : %f, dual objective : %f, dual_gap : %f\" % (objective_function, dual_objective_function, gap))\n",
    "\n",
    "            if gap<=tol and i>10:\n",
    "                print(\"convergence at iteration : %d\" %i)\n",
    "                break\n",
    "\n",
    "        if verbose:\n",
    "            print(\"dual gap : %f\" % gap)\n",
    "            print(\"objective_function : %f\" % objective_function)\n",
    "            print(\"dual_objective_function : %f\" % dual_objective_function)\n",
    "            print(\"dual_penalisation : %f\" % dual_penalisation)\n",
    "            print(\"dual_loss : %f\" % dual_loss)\n",
    "        self.coefs_ = coefs_next\n",
    "        self.gap = gap\n",
    "        self.objective_function = objective_function\n",
    "        self.dual_objective_function = dual_objective_function\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, K):\n",
    "        \"\"\" Returns the prediction associated to the Kernels represented by K\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        K : ndarray \n",
    "            ndarray of size (n_samples, n_kernels*n_samples) representing the kernels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray : the prediction associated to K\n",
    "        \"\"\"\n",
    "        if self.loss=='squared-hinge':\n",
    "            res = np.sign(np.dot(K, self.coefs_))\n",
    "            res[res==0] = 1\n",
    "            return res\n",
    "        else:\n",
    "            return np.dot(K, self.coefs_)\n",
    "\n",
    "    def score(self, K, y):\n",
    "        \"\"\" Returns the score prediction for the given data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        K : ndarray\n",
    "            matrix of observations\n",
    "\n",
    "        y : ndarray\n",
    "            the labels correspondings to K\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The percentage of good classification for K\n",
    "        \"\"\"\n",
    "        if self.loss=='squared-hinge':\n",
    "            return np.sum(np.equal(self.predict(K), y))*100./len(y)\n",
    "        else:\n",
    "            print(\"Score not yet implemented for regression\\n\")\n",
    "            return None\n",
    "\n",
    "    def info(self, K, y):\n",
    "        \"\"\" For test purpose\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        K : 2D-array\n",
    "            kernels\n",
    "\n",
    "        y : ndarray\n",
    "            labels\n",
    "        Returns\n",
    "        -------\n",
    "        A dict of informations\n",
    "        \"\"\"\n",
    "        result = Bunch()\n",
    "        n_samples, n_kernels = self.n_samples, self.n_kernels\n",
    "        nulled_kernels = 0\n",
    "        nulled_coefs_per_kernel = list()\n",
    "\n",
    "        for i in self.coefs_.reshape(n_kernels, n_samples):\n",
    "            if len(i[i!=0]) == 0:\n",
    "                nulled_kernels = nulled_kernels + 1\n",
    "            nulled_coefs_per_kernel.append(len(i[i==0]))\n",
    "\n",
    "        result['score'] = self.score(K, y)\n",
    "        result['norms'] = by_kernel_norm(self.coefs_, self.p, self.q,\n",
    "                n_samples, n_kernels)\n",
    "        result['nulled_coefs'] = len(self.coefs_[self.coefs_==0])\n",
    "        result['nulled_kernels'] = nulled_kernels\n",
    "        result['nulled_coefs_per_kernel'] = nulled_coefs_per_kernel\n",
    "        result['objective_function'] = self.objective_function\n",
    "        result['dual_objective_function'] = self.dual_objective_function\n",
    "        result['gap'] = self.gap\n",
    "        result['auc_score'] = roc_auc_score(y, self.predict(K))\n",
    "        result['lambda_'] = self.lambda_\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device('cuda')\n",
    "#    torch.backends.cudnn.enabled = False\n",
    "#else:\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_padding(list_of_embeddings, length, embedding_length):\n",
    "    zero_vec = np.zeros(embedding_length)\n",
    "    for _ in range(length - len(list_of_embeddings)):\n",
    "        list_of_embeddings.append(zero_vec)\n",
    "    return list_of_embeddings[:length]\n",
    "\n",
    "\n",
    "def word2vec_sum(list_of_embeddings, embedding_length):\n",
    "    ret_value = np.zeros(embedding_length)\n",
    "    for embedding in list_of_embeddings:\n",
    "        ret_value += embedding\n",
    "    return ret_value\n",
    "\n",
    "class OneHotParser(TextParser):\n",
    "    def __init__(self, vocabulary, type_func):\n",
    "        \"\"\"\n",
    "\n",
    "        :param vocabulary: List of strings representing the vocabulary.\n",
    "        :param type_func: Function which converts the output to the desired type, e.g. np.array.\n",
    "        \"\"\"\n",
    "        self.vocab = vocabulary\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        TextParser.__init__(self, type_func)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        one_hot = np.zeros((len(x), self.vocab_size))  # +1 for out of vocabulary tokens.\n",
    "        for idx, token_list in enumerate(x):\n",
    "            sentence = ' '.join(token_list)\n",
    "            vocab_idx = self.vocab.index(sentence)\n",
    "            one_hot[idx, vocab_idx] = 1\n",
    "\n",
    "        return self.convert_type(one_hot)\n",
    "\n",
    "def load_list_from_file(file_path):\n",
    "    with open(file_path) as file:\n",
    "        content = file.readlines()\n",
    "    ret = []\n",
    "    for elem in content:\n",
    "        clean_elem = elem.strip()\n",
    "        if len(clean_elem) > 0:\n",
    "            ret.append(clean_elem)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'full'\n",
    "with open(os.getcwd() + '/data/zork_walkthrough_' + task + '.txt', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "raw_actions = data['actions']\n",
    "raw_states = data['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = ['go', 'take', 'open', 'grab', 'run', 'walk', 'climb', 'kill', 'light', 'get']\n",
    "\n",
    "#basic_actions = ['open', 'egg', 'east', 'west', 'north', 'south', 'go', 'up', 'down', 'look', 'take']\n",
    "basic_actions = ['open', 'egg', 'north', 'climb', 'tree', 'take']\n",
    "\n",
    "extended_actions = ['grab', 'run', 'climb', 'walk', 'go', 'south', 'east', 'west']\n",
    "\n",
    "basic_objects = ['egg', 'door', 'tree', 'leaves', 'nest']\n",
    "\n",
    "obj_ext1 = ['bag', 'bottle', 'rope', 'sword', 'lantern', 'knife', 'mat', 'mailbox',\n",
    "            'rug', 'case', 'axe', 'diamond', 'leaflet', 'news', 'brick']\n",
    "action_ext1 = ['enter', 'open the window', 'turn lamp on', 'move rug', 'open trap door', 'hit troll with sword']\n",
    "\n",
    "random_words = ['bring', 'wait', 'test', 'heave', 'squat', 'garbage', 'you', 'no', 'year']\n",
    "\n",
    "def create_actions():\n",
    "    action_vocabulary = {}\n",
    "    for word in dictionary:\n",
    "        action_vocabulary[word] = word2vec_model[word]\n",
    "\n",
    "    embedding_size = len(action_vocabulary['open'])\n",
    "    \n",
    "    return action_vocabulary, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = glove_api.load('glove-wiki-gigaword-50')\n",
    "embedding_size = word2vec_model.vector_size\n",
    "word2vec_parser = Word2Vec(type_func=lambda x: torch.FloatTensor(x).to(device).unsqueeze(0),\n",
    "                           word2vec_model=word2vec_model,\n",
    "                           return_func=lambda x: word2vec_padding(x, 65, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fista = Fista(lambda_=0.8, loss='least-square', penalty='l11', n_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = ['pray', 'yellow', 'trapdoor', 'open', 'bell', 'touch', 'pile', 'trunk', 'sack', 'inflate', 'southeast',\n",
    "#               'of', 'move', 'match', 'figurine', 'railing', 'with', 'map', 'mirror', 'wind', 'examine', 'north', 'out',\n",
    "#               'trident', 'turn', 'skull', 'throw', 'northwest', 'case', 'bag', 'red', 'press', 'jewels', 'east', 'pump',\n",
    "#               'bolt', 'rusty', 'window', 'douse', 'boat', 'bracelet', 'matchbook', 'basket', 'book', 'coffin', 'bar',\n",
    "#               'rug', 'lid', 'drop', 'nasty', 'wrench', 'light', 'sand', 'bauble', 'kill', 'tie', 'painting', 'sword',\n",
    "#               'wave', 'in', 'south', 'northeast', 'ring', 'canary', 'lower', 'egg', 'all', 'to', 'candles', 'page',\n",
    "#               'and', 'echo', 'emerald', 'tree', 'from', 'rope', 'troll', 'screwdriver', 'torch', 'enter', 'coal', 'go',\n",
    "#               'look', 'shovel', 'knife', 'down', 'take', 'switch', 'prayer', 'launch', 'diamond', 'read', 'up', 'get',\n",
    "#               'scarab', 'west', 'land', 'southwest', 'climb', 'thief', 'raise', 'wait', 'odysseus', 'button', 'sceptre',\n",
    "#               'lamp', 'chalice', 'garlic', 'buoy', 'pot', 'label', 'put', 'dig', 'machine', 'close', 'walk', 'run', 'hit', 'attack']\n",
    "ambiguities = {'go': ['move', 'walk', 'run'], 'get': ['take'], 'kill': ['hit', 'attack'], 'press': ['push'], 'put': ['place'], 'drop': ['toss']}\n",
    "dictionary = ['all', 'and', 'attack', 'bag', 'bar', 'basket', 'bauble', 'bell', 'boat', 'bolt', 'book', 'bracelet', 'buoy', 'button', 'canary', 'candles', 'case', 'chalice', 'close', 'coal', 'coffin', 'diamond', 'dig', 'douse', 'down', 'drop', 'east', 'echo', 'egg', 'emerald', 'enter', 'examine', 'figurine', 'from', 'garlic', 'get', 'hit', 'in', 'inflate', 'jewels', 'kill', 'knife', 'label', 'lamp', 'land', 'launch', 'lid', 'light', 'look', 'lower', 'machine', 'map', 'match', 'matchbook', 'mirror', 'move', 'nasty', 'north', 'northeast', 'northwest', 'odysseus', 'of', 'open', 'out', 'page', 'painting', 'pile', 'place', 'pot', 'pray', 'prayer', 'press', 'pump', 'push', 'put', 'railing', 'raise', 'read', 'red', 'ring', 'rope', 'rug', 'run', 'rusty', 'sack', 'sand', 'scarab', 'sceptre', 'screwdriver', 'shovel', 'skull', 'south', 'southeast', 'southwest', 'switch', 'sword', 'take', 'thief', 'throw', 'tie', 'to', 'torch', 'toss', 'touch', 'trapdoor', 'trident', 'troll', 'trunk', 'turn', 'up', 'wait', 'walk', 'wave', 'west', 'wind', 'window', 'with', 'wrench', 'yellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = MultivariateNormal(torch.zeros(50), torch.eye(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(additional_prints, threshold):\n",
    "\n",
    "    accurate = 0\n",
    "\n",
    "    full_state = torch.zeros((agent.history_size,\n",
    "                                  2,\n",
    "                                  agent.input_width,\n",
    "                                  agent.input_length), dtype=torch.float32).to(agent.device)\n",
    "    for idx in range(len(raw_actions)):\n",
    "        obs = raw_states[idx]\n",
    "        obs = agent._parse_state(obs).view(2, agent.input_width, agent.input_length)\n",
    "        full_state[:agent.history_size - 1] = full_state[1:]\n",
    "        full_state[-1] = obs\n",
    "\n",
    "        deepcs_output = F.sigmoid(network(full_state.unsqueeze(0)).squeeze(0))\n",
    "        list_of_words = []\n",
    "        output_str = ''\n",
    "        for index in range(len(deepcs_output)):\n",
    "            if deepcs_output[index] > threshold:\n",
    "                has_amb = False\n",
    "                for key in ambiguities:\n",
    "                    if dictionary[index] in ambiguities[key]:\n",
    "                        list_of_words.append(key)\n",
    "                        has_amb = True\n",
    "                        break\n",
    "                if not has_amb:\n",
    "                    list_of_words.append(dictionary[index])\n",
    "            if additional_prints:\n",
    "                output_str += ' ' + dictionary[index] + ': ' + str(deepcs_output[index].cpu().item())\n",
    "\n",
    "        if set(tokenizer(raw_actions[idx])) == set(list_of_words):\n",
    "            accurate += 1\n",
    "        if additional_prints:\n",
    "            print(output_str)\n",
    "            print(list_of_words)\n",
    "            print(tokenizer(raw_actions[idx]))\n",
    "\n",
    "    return accurate * 1.0 / len(raw_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(additional_prints, threshold, seed=12):\n",
    "    with torch.no_grad():            \n",
    "        obs = agent.env.reset(seed)\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        idx = 0\n",
    "        full_state = torch.zeros((agent.history_size,\n",
    "                                  2,\n",
    "                                  agent.input_width,\n",
    "                                  agent.input_length), dtype=torch.float32).to(agent.device)\n",
    "\n",
    "        while not done:\n",
    "            obs = agent._parse_state(obs).view(2, agent.input_width, agent.input_length)\n",
    "            full_state[:agent.history_size - 1] = full_state[1:]\n",
    "            full_state[-1] = obs\n",
    "\n",
    "            deepcs_output = network(obs.unsqueeze(0)).squeeze(0)\n",
    "            list_of_words = []\n",
    "            for idx in range(len(deepcs_output)):\n",
    "                if deepcs_output[idx] > threshold:\n",
    "                    list_of_words.append(idx)\n",
    "\n",
    "            _, text_command = agent._select_eps_greedy_action(0, list_of_words, None)\n",
    "\n",
    "            if additional_prints:\n",
    "                agent.env.render()\n",
    "                print(text_command)\n",
    "            idx += 1\n",
    "            obs, rew, done, has_won = agent.env.step(text_command)\n",
    "            if additional_prints:\n",
    "                print(rew)\n",
    "            reward += rew - 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return int(agent.env.env.get_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_neighbors=5\n",
    "\n",
    "action_vocabulary, embedding_size = create_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OMPDDPG(actions=action_vocabulary,\n",
    "                state_parser=word2vec_parser,\n",
    "                embedding_size=embedding_size,\n",
    "                input_length=embedding_size,\n",
    "                input_width=65,\n",
    "                history_size=12,\n",
    "                model_type='CNN',\n",
    "                device=device,\n",
    "                pomdp_mode=False,\n",
    "                loss_weighting=1.0,\n",
    "                linear=False,\n",
    "                improved_omp=False,\n",
    "                task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.sparse_reward = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/deep_cs_' + task + '_full_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TextCNN(embedding_size=embedding_size, history_size=agent.history_size, input_width=65,\n",
    "                            output_size=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "0.9926289926289926\n",
      "0.5\n",
      "0.9926289926289926\n",
      "0.7\n",
      "0.9926289926289926\n",
      "0.9\n",
      "0.9926289926289926\n"
     ]
    }
   ],
   "source": [
    "full_path = path + '0.0_0.0/0/20000'\n",
    "network.load_state_dict(torch.load(full_path + '/network'))\n",
    "for threshold in [0.3, 0.5, 0.7, 0.9]:\n",
    "    print(threshold)\n",
    "    print(test_accuracy(False, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((4, 6, 6))\n",
    "for idx0, threshold in enumerate([0.3, 0.5, 0.7, 0.9]):\n",
    "    for idx1, prob in enumerate([0.0, 0.1, 0.2, 0.3, 0.4, 0.5]):\n",
    "        for idx2, amb in enumerate([0.0, 0.1, 0.2, 0.3, 0.4, 0.5]):\n",
    "            full_path = path + str(prob) + '_' + str(amb) + '/0/20000'\n",
    "            network.load_state_dict(torch.load(full_path + '/network'))\n",
    "            results[idx0, idx1, idx2] = test_accuracy(False, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.99262899 0.99262899 0.99262899 0.99262899 0.99262899 0.99262899]\n",
      "  [0.99262899 0.99262899 0.99262899 0.99262899 0.99262899 0.99262899]\n",
      "  [0.99262899 0.99262899 0.99262899 0.99262899 0.99262899 0.99017199]\n",
      "  [0.99262899 0.99262899 0.99262899 0.99017199 0.99262899 0.99262899]\n",
      "  [0.99262899 0.99262899 0.99262899 0.99017199 0.99262899 0.98034398]\n",
      "  [0.99262899 0.99017199 0.97788698 0.94348894 0.98525799 0.98034398]]\n",
      "\n",
      " [[0.99262899 0.99262899 0.99262899 0.99262899 0.97788698 0.93857494]\n",
      "  [0.99262899 0.99262899 0.99262899 0.98525799 0.92628993 0.86240786]\n",
      "  [0.99262899 0.99017199 0.98771499 0.94348894 0.83292383 0.77395577]\n",
      "  [0.97542998 0.97297297 0.92137592 0.84275184 0.73464373 0.84029484]\n",
      "  [0.85503686 0.88697789 0.8992629  0.73464373 0.73218673 0.85012285]\n",
      "  [0.48894349 0.57002457 0.64619165 0.71007371 0.74447174 0.88206388]]\n",
      "\n",
      " [[0.99262899 0.99262899 0.98771499 0.83046683 0.72727273 0.72727273]\n",
      "  [0.98034398 0.96560197 0.87960688 0.78378378 0.72727273 0.72727273]\n",
      "  [0.86732187 0.87960688 0.77641278 0.72727273 0.72481572 0.72235872]\n",
      "  [0.38329238 0.4987715  0.54545455 0.63390663 0.68304668 0.71007371]\n",
      "  [0.03931204 0.13759214 0.32186732 0.46928747 0.56511057 0.66339066]\n",
      "  [0.         0.01719902 0.03685504 0.17199017 0.38820639 0.53071253]]\n",
      "\n",
      " [[0.99262899 0.85995086 0.72727273 0.72727273 0.72727273 0.72727273]\n",
      "  [0.49385749 0.54054054 0.51597052 0.63390663 0.63882064 0.67813268]\n",
      "  [0.00982801 0.03931204 0.08845209 0.16461916 0.25061425 0.4029484 ]\n",
      "  [0.         0.         0.         0.002457   0.04668305 0.11793612]\n",
      "  [0.         0.         0.         0.         0.         0.00737101]\n",
      "  [0.         0.         0.         0.         0.         0.002457  ]]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_results = np.array([[1.   ,      1.     ,    1.    ,     1.  ,       1.    ,     1.        ],\n",
    " [1.   ,      1.    ,     1.    ,     1.     ,    1.   ,      1.        ],\n",
    " [0.97788698 ,0.97788698, 0.97788698, 0.97788698, 0.98034398, 0.97788698],\n",
    " [0.87714988, 0.87714988, 0.87469287, 0.87469287, 0.87714988, 0.88206388],\n",
    " [0.77149877, 0.75921376, 0.76412776, 0.76658477, 0.77149877, 0.76658477],\n",
    " [0.51842752, 0.5012285 , 0.53562654, 0.5036855,  0.54791155, 0.50859951]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# corr = np.corrcoef(np.random.randn(10, 200))\n",
    "mask = np.zeros_like(results[0])\n",
    "ticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(cs_results[::-1,:] - results[2,::-1,:], mask=mask, vmin=0, vmax=1, square=True,  cmap=\"YlGnBu\", xticklabels=ticks, yticklabels=ticks[::-1])\n",
    "    ax.set_xlabel('Probability of seeing a synonym', fontsize=15)\n",
    "    ax.set_ylabel('Imperfect action probability', fontsize=15)\n",
    "    ax.set_title('Accuracy(IK-OMP) - Accuracy(Deep CS)', fontsize=20, pad=20)\n",
    "    plt.show()\n",
    "    \n",
    "#     ax = sns.heatmap(cs_results[::-1,:], mask=mask, vmin=0, vmax=1, square=True,  cmap=\"YlGnBu\", xticklabels=ticks, yticklabels=ticks[::-1])\n",
    "#     ax.set_xlabel('Ambiguitiy Probability')\n",
    "#     ax.set_ylabel('Imperfect Probability')\n",
    "#     ax.set_title('IK-OMP')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results for safe keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_reward = {0.3: {0.0: [35, 35], 0.05: [35, 35], 0.1: [35, 35], 0.15: [35, 35], 0.2: [35, 35], 0.25: [35, 35], 0.3: [35, 35], 0.35: [35, 35], 0.4: [10, 0], 0.45: [0, 0], 0.5: [0, 0]}, 0.5: {0.0: [35, 35], 0.05: [35, 35], 0.1: [35, 35], 0.15: [35, 35], 0.2: [35, 35], 0.25: [35, 35], 0.3: [35, 35], 0.35: [10, 0], 0.4: [0, 0], 0.45: [0, 0], 0.5: [0, 0]}, 0.7: {0.0: [35, 35], 0.05: [35, 35], 0.1: [35, 35], 0.15: [35, 35], 0.2: [35, 35], 0.25: [35, 35], 0.3: [10, 0], 0.35: [0, 0], 0.4: [0, 0], 0.45: [0, 0], 0.5: [0, 0]}, 0.9: {0.0: [35, 35], 0.05: [35, 35], 0.1: [35, 35], 0.15: [35, 35], 0.2: [35, 35], 0.25: [10, 10], 0.3: [0, 0], 0.35: [0, 0], 0.4: [0, 0], 0.45: [0, 0], 0.5: [0, 0]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_reward = {}\n",
    "probs = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\n",
    "for threshold in [0.3, 0.5, 0.7, 0.9]:\n",
    "    results_reward[threshold] = {}\n",
    "    for prob in probs:\n",
    "        results_reward[threshold][prob] = []\n",
    "\n",
    "for subdir in range(5):\n",
    "    for prob in probs:\n",
    "        for threshold in [0.3, 0.5, 0.7, 0.9]:\n",
    "            print(str(subdir) + ' ' + str(prob) + ' ' + str(threshold))\n",
    "            full_path = path + str(prob) + '/' + str(subdir) + '/20000'\n",
    "            network.load_state_dict(torch.load(full_path + '/network'))\n",
    "            results_reward[threshold][prob].append(test_env(False, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(results_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#396ab1', '#da7c30', '#3e9651', '#cc2529', '#94823d', '#535154', '#006400', '#00FF00', '#800000', '#F08080', '#FFFF00', '#000000', '#C0C0C0']\n",
    "facecolors = ['#7293cb', '#e1974c', '#84ba5b', '#d35e60', '#ccc210', '#808585']\n",
    "\n",
    "f, axarr = pl.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "idx = 0\n",
    "for test_name in results_reward:\n",
    "    avg = [res[0] for res in results_reward[test_name]]\n",
    "    std = [res[1] for res in results_reward[test_name]]\n",
    "    pl.plot(snr, avg, label=test_name, color=colors[idx])\n",
    "    pl.fill_between(snr, np.array(avg) - np.array(std), np.array(avg) + np.array(std), facecolor=facecolors[idx], alpha=0.2, interpolate=True)\n",
    "    idx += 1\n",
    "\n",
    "leg = pl.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), shadow=True, ncol=3, fontsize=10)\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(3.0)\n",
    "    \n",
    "#pl.suptitle('Egg Quest, Minimal Action Set,\\n GloVe, Training with K=all', fontsize=20, y=1.1)\n",
    "pl.xlabel('SnR')\n",
    "pl.ylabel('Reward')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence = 0\n",
    "for w1 in dictionary:\n",
    "    for w2 in dictionary:\n",
    "        if w1 != w2:\n",
    "            top = np.abs(np.sum(word2vec_model[w1] * word2vec_model[w2]))\n",
    "            bottom = np.sqrt(np.sum(word2vec_model[w1] * word2vec_model[w1]) * np.sum(word2vec_model[w2] * word2vec_model[w2]))\n",
    "            couple_coherence = top/bottom\n",
    "            coherence = max(coherence, couple_coherence)\n",
    "print(coherence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
