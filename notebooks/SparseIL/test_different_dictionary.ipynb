{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import gensim\n",
    "import gensim.downloader as glove_api\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as pl\n",
    "import pickle\n",
    "\n",
    "from ZorkGym.text_utils.text_parser import BagOfWords, Word2Vec, TextParser, tokenizer\n",
    "from agents.OMP_DDPG import OMPDDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda')\n",
    "#     torch.backends.cudnn.enabled = False\n",
    "# else:\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_padding(list_of_embeddings, length, embedding_length):\n",
    "    zero_vec = np.zeros(embedding_length)\n",
    "    for _ in range(length - len(list_of_embeddings)):\n",
    "        list_of_embeddings.append(zero_vec)\n",
    "    return list_of_embeddings[:length]\n",
    "\n",
    "\n",
    "def word2vec_sum(list_of_embeddings, embedding_length):\n",
    "    ret_value = np.zeros(embedding_length)\n",
    "    for embedding in list_of_embeddings:\n",
    "        ret_value += embedding\n",
    "    return ret_value\n",
    "\n",
    "class OneHotParser(TextParser):\n",
    "    def __init__(self, vocabulary, type_func):\n",
    "        \"\"\"\n",
    "\n",
    "        :param vocabulary: List of strings representing the vocabulary.\n",
    "        :param type_func: Function which converts the output to the desired type, e.g. np.array.\n",
    "        \"\"\"\n",
    "        self.vocab = vocabulary\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        TextParser.__init__(self, type_func)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        one_hot = np.zeros((len(x), self.vocab_size))  # +1 for out of vocabulary tokens.\n",
    "        for idx, token_list in enumerate(x):\n",
    "            sentence = ' '.join(token_list)\n",
    "            vocab_idx = self.vocab.index(sentence)\n",
    "            one_hot[idx, vocab_idx] = 1\n",
    "\n",
    "        return self.convert_type(one_hot)\n",
    "\n",
    "def load_list_from_file(file_path):\n",
    "    with open(file_path) as file:\n",
    "        content = file.readlines()\n",
    "    ret = []\n",
    "    for elem in content:\n",
    "        clean_elem = elem.strip()\n",
    "        if len(clean_elem) > 0:\n",
    "            ret.append(clean_elem)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = ['go', 'take', 'open', 'grab', 'run', 'walk', 'climb']\n",
    "vocabulary = load_list_from_file('./data/vocabulary.txt')\n",
    "\n",
    "#basic_actions = ['open', 'egg', 'east', 'west', 'north', 'south', 'go', 'up', 'down', 'look', 'take']\n",
    "basic_actions = ['open', 'egg', 'north', 'climb', 'tree', 'take']\n",
    "\n",
    "extended_actions = ['grab', 'run', 'climb', 'walk', 'go', 'south', 'east', 'west']\n",
    "\n",
    "basic_objects = ['egg', 'door', 'tree', 'leaves', 'nest']\n",
    "\n",
    "obj_ext1 = ['bag', 'bottle', 'rope', 'sword', 'lantern', 'knife', 'mat', 'mailbox',\n",
    "            'rug', 'case', 'axe', 'diamond', 'leaflet', 'news', 'brick']\n",
    "action_ext1 = ['enter', 'open the window', 'turn lamp on', 'move rug', 'open trap door', 'hit troll with sword']\n",
    "\n",
    "random_words = ['bring', 'wait', 'test', 'heave', 'squat', 'garbage', 'you', 'no', 'year']\n",
    "\n",
    "def create_actions():\n",
    "    actions = list(basic_actions)\n",
    "    if task == 1:\n",
    "        actions = list(basic_actions) + list(extended_actions)\n",
    "\n",
    "    words = list()\n",
    "    words.append('')\n",
    "    for action in actions:\n",
    "        tokens = tokenizer(action)\n",
    "        for token in tokens:\n",
    "            if token not in words:\n",
    "                words.append(token)\n",
    "\n",
    "    sentences = list()\n",
    "    for i, word1 in enumerate(words):\n",
    "        for word2 in words[i + 1:]:\n",
    "            if word1 in verbs:\n",
    "                sentences.append(word1 + ' ' + word2)\n",
    "            else:\n",
    "                sentences.append(word2 + ' ' + word1)\n",
    "                \n",
    "    words = set()\n",
    "    for action in sentences:\n",
    "        for word in tokenizer(action):\n",
    "            words.add(word)\n",
    "    action_vocabulary = {}\n",
    "    if True: #action_w2v:\n",
    "        for word in words:\n",
    "            action_vocabulary[word] = word2vec_model[word]\n",
    "        action_vocabulary[''] = [0 for _ in range(len(action_vocabulary['open']))]\n",
    "    else:\n",
    "        words.add('')\n",
    "        for idx, word in enumerate(words):\n",
    "            action_vocabulary[word] = np.zeros(len(words))\n",
    "            action_vocabulary[word][idx] = 1.0\n",
    "\n",
    "    embedding_size = len(action_vocabulary['open'])\n",
    "    \n",
    "    return actions, action_vocabulary, embedding_size, words, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(vocabulary)\n",
    "# bow_parser = BagOfWords(vocabulary=vocabulary,\n",
    "#                         type_func=lambda x: torch.FloatTensor(x).to(device).unsqueeze(1))\n",
    "\n",
    "word2vec_model = glove_api.load('glove-wiki-gigaword-50')\n",
    "embedding_size = word2vec_model.vector_size\n",
    "word2vec_parser = Word2Vec(type_func=lambda x: torch.FloatTensor(x).to(device).unsqueeze(0),\n",
    "                           word2vec_model=word2vec_model,\n",
    "                           return_func=lambda x: word2vec_padding(x, 65, embedding_size))\n",
    "# onehot_parser = OneHotParser(type_func=lambda x: torch.FloatTensor(x).to(device).unsqueeze(1),\n",
    "#                              vocabulary=actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(additional_prints, test_iterations):\n",
    "    total_reward = 0\n",
    "    iteration = 0\n",
    "    with torch.no_grad():\n",
    "        while iteration < test_iterations:\n",
    "            try:\n",
    "                obs = agent.env.reset()\n",
    "                done = False\n",
    "\n",
    "                full_state = torch.zeros((agent.history_size,\n",
    "                                          2,\n",
    "                                          agent.input_width,\n",
    "                                          agent.input_length), dtype=torch.float32).to(agent.device)\n",
    "\n",
    "                episode_reward = 0\n",
    "                while not done:\n",
    "                    obs = agent._parse_state(obs).view(2, agent.input_width, agent.input_length)\n",
    "                    full_state[:agent.history_size - 1] = full_state[1:]\n",
    "                    full_state[-1] = obs\n",
    "\n",
    "                    action, text_command = agent._get_action(full_state.unsqueeze(0),\n",
    "                                                            tau=0,\n",
    "                                                            eps=0,\n",
    "                                                            test=True,\n",
    "                                                            additional_prints=False,\n",
    "                                                            number_of_neighbors=number_of_neighbors)\n",
    "                    if additional_prints:\n",
    "                        agent.env.render()\n",
    "                        print(text_command)\n",
    "                        print(action)\n",
    "                        print(agent._get_q_value(agent.network,\n",
    "                                                full_state.unsqueeze(0),\n",
    "                                                action))\n",
    "\n",
    "                    obs, reward, done, has_won = agent.env.step(text_command)\n",
    "\n",
    "                    episode_reward += reward\n",
    "\n",
    "                if additional_prints:\n",
    "                    agent.env.render()\n",
    "\n",
    "                total_reward += episode_reward\n",
    "                iteration += 1\n",
    "            except EnvironmentError:\n",
    "                print('There was some issue with the Zork test env.')\n",
    "\n",
    "    return total_reward * 1.0 / test_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 1\n",
    "path = '/home/deep/ZorkDiscreteDDPG/egg_quest_baby_actions/ompddpg_cnn/neighbors=-1/w2v/omp_ddpg_50_-1_mse_loss/0/'\n",
    "number_of_neighbors=1\n",
    "\n",
    "actions, action_vocabulary, embedding_size, words, sentences= create_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(word)\n",
    "    print(word2vec_model[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OMPDDPG(actions=action_vocabulary,\n",
    "                state_parser=word2vec_parser,\n",
    "                embedding_size=embedding_size,\n",
    "                input_length=embedding_size,\n",
    "                input_width=65,\n",
    "                history_size=1,\n",
    "                model_type='CNN',\n",
    "                device=device,\n",
    "                pomdp_mode=False,\n",
    "                loss_weighting=1.0,\n",
    "                linear=False,\n",
    "                improved_omp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data + simple plot to make sure data is OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dirs = sorted([int(d) for d in os.listdir(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + str(sub_dirs[-1]) + '/results', 'rb') as f:\n",
    "    baseline = np.array(pickle.load(f)['rewards']['nn=-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = pl.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "pl.plot(baseline[:,0], baseline[:,1])\n",
    "    \n",
    "pl.xlabel('Time step')\n",
    "pl.ylabel('Reward')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = []\n",
    "for sub_dir in sub_dirs:\n",
    "    agent.network[0].load_state_dict(torch.load(path + str(sub_dir) + '/actor'))\n",
    "    agent.network[1].load_state_dict(torch.load(path + str(sub_dir) + '/critic'))\n",
    "    baseline_results.append(test(False, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = pl.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "pl.plot(baseline[:,0], baseline[:,1], label='Baseline (collected)')\n",
    "pl.plot(baseline[:,0], baseline_results, label='Baseline (tested)')\n",
    "\n",
    "leg = pl.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), shadow=True, ncol=2, fontsize=10)\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(3.0)\n",
    "\n",
    "pl.xlabel('Time step')\n",
    "pl.ylabel('Reward')\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary with synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 1\n",
    "number_of_neighbors=1\n",
    "\n",
    "actions, action_vocabulary, embedding_size, words, sentences = create_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OMPDDPG(actions=action_vocabulary,\n",
    "                state_parser=word2vec_parser,\n",
    "                embedding_size=embedding_size,\n",
    "                input_length=embedding_size,\n",
    "                input_width=65,\n",
    "                history_size=1,\n",
    "                model_type='CNN',\n",
    "                device=device,\n",
    "                pomdp_mode=False,\n",
    "                loss_weighting=1.0,\n",
    "                linear=False,\n",
    "                improved_omp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for sub_dir in sub_dirs:\n",
    "    agent.network[0].load_state_dict(torch.load(path + str(sub_dir) + '/actor'))\n",
    "    agent.network[1].load_state_dict(torch.load(path + str(sub_dir) + '/critic'))\n",
    "    results.append(test(False, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = pl.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "pl.plot(baseline[:,0], baseline_results, label='Baseline (collected)')\n",
    "pl.plot(baseline[:,0], results, label='Synonyms')\n",
    "\n",
    "leg = pl.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), shadow=True, ncol=2, fontsize=10)\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(3.0)\n",
    "\n",
    "pl.xlabel('Time step')\n",
    "pl.ylabel('Reward')\n",
    "\n",
    "pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
